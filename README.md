
# TinyLlama Fine-Tuning Toolkit (English Section)

---

## ğŸŒ Overview

This repository provides a **compact, end-to-end fine-tuning, serving, and evaluation toolkit**  
for **TinyLlama-1.1B-Chat-v1.0**, enabling reproducible experiments across:

- Full Fine-Tuning (FT)  
- LoRA  
- QLoRA  
- vLLM serving  
- RAG evaluation  
- Unified model comparison utilities  

Prefix Tuning has been intentionally excluded in this version to keep the stack clean and minimal.

---

## ğŸ—ï¸ Architecture & Tech Stack

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Training Layer             â”‚
â”‚  â€¢ Full FT (HF Trainer)                   â”‚
â”‚  â€¢ LoRA (PEFT)                            â”‚
â”‚  â€¢ QLoRA (PEFT + 4bit Quantization)       â”‚
â”‚                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               Model Outputs                â”‚
â”‚  models/ft_full/                           â”‚
â”‚  models/ft_lora/                           â”‚
â”‚  models/ft_qlora/                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Serving & Evaluation            â”‚
â”‚  â€¢ vLLM OpenAI-compatible server           â”‚
â”‚  â€¢ app_rag_compare.py (RAG pipeline)       â”‚
â”‚  â€¢ compare_adapters.py                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“š Fine-Tuning Methods

### **1. Full Fine-Tuning**
All model parameters are updated via supervised fine-tuning (SFT).  
This produces the highest capacity and quality, but requires significant GPU resources.

Run:
```bash
python train_full.py
```

---

### **2. LoRA (Low-Rank Adaptation)**
LoRA injects trainable low-rank matrices into attention projections,  
training only these additional parameters while freezing the base model.

Characteristics:
- Lightweight  
- Fast  
- Extremely memory efficient  
- Adapter weights are tiny

Run:
```bash
python train_lora.py
```

---

### **3. QLoRA**
QLoRA quantizes the model backbone to **4-bit NF4**,  
while training LoRA adapters in fp16, drastically reducing VRAM usage.

Characteristics:
- Lowest VRAM consumption  
- Works on 8â€“16GB GPUs  
- Near-LoRA quality

Run:
```bash
python train_qlora.py
```

---

## ğŸ§ª Dataset

SFT demo dataset is located at:

`data/toy_qa.jsonl`

Format:
```json
{"question": "...", "answer": "..."}
```

---

## ğŸš€ Serving with vLLM

vLLM provides high-throughput inference through PagedAttention.

Run:
```bash
python -m vllm.entrypoints.openai.api_server --model ./models/ft_full
```

---

## ğŸ” RAG Evaluation

Run:
```bash
python app_rag_compare.py --docs_dir docs --question "Explain LoRA."
```

Evaluates:
- Embedding quality  
- Retrieval differences  
- Answer consistency  

---

## ğŸ§­ Model Comparison

Run:
```bash
python compare_adapters.py
```

Compares output for:
- Full FT  
- LoRA  
- QLoRA  

---

## ğŸ“ Repository Structure

```
llm_ft_tinyllama/
â”œâ”€â”€ train_full.py
â”œâ”€â”€ train_lora.py
â”œâ”€â”€ train_qlora.py
â”œâ”€â”€ compare_adapters.py
â”œâ”€â”€ app_rag_compare.py
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ ft_full/
â”‚   â”œâ”€â”€ ft_lora/
â”‚   â””â”€â”€ ft_qlora/
â”‚
â””â”€â”€ data/
    â””â”€â”€ toy_qa.jsonl
```

---

## ğŸ›  Requirements

- Python 3.10+  
- PyTorch (CUDA)  
- HuggingFace Transformers  
- PEFT  
- bitsandbytes  
- vLLM  

Install:
```bash
pip install -r requirements.txt
```

---

## ğŸ™Œ Final Notes

This repository is intended to be a **clean, extensible baseline** for LLM fine-tuning research.

---

# TinyLlama å¾®èª¿æ•´ãƒ„ãƒ¼ãƒ«ã‚­ãƒƒãƒˆï¼ˆæ—¥æœ¬èªã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼‰

---

## ğŸŒ æ¦‚è¦

æœ¬ãƒªãƒã‚¸ãƒˆãƒªã¯ã€**TinyLlama-1.1B-Chat-v1.0 ã®å¾®èª¿æ•´ãƒ»æ¨è«–ãƒ»è©•ä¾¡ã‚’ä¸€æ°—é€šè²«ã§æ‰±ãˆã‚‹ã€æœ€å°æ§‹æˆã®å®Ÿé¨“ç”¨ã‚­ãƒƒãƒˆ**ã§ã™ã€‚

ä»¥ä¸‹ã®æ§‹æˆã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã™ï¼š

- ãƒ•ãƒ«å¾®èª¿æ•´ï¼ˆFull FTï¼‰  
- LoRA  
- QLoRA  
- vLLM ã«ã‚ˆã‚‹é«˜é€Ÿæ¨è«–  
- RAG è©•ä¾¡  
- ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£  

Prefix Tuning ã¯æœ¬ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‹ã‚‰é™¤å¤–æ¸ˆã¿ã§ã™ã€‚

---

## ğŸ—ï¸ æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯ãƒ»ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                å­¦ç¿’ãƒ¬ã‚¤ãƒ¤ãƒ¼               â”‚
â”‚  â€¢ Full FT (HF Trainer)                   â”‚
â”‚  â€¢ LoRA (PEFT)                            â”‚
â”‚  â€¢ QLoRA (4bit é‡å­åŒ– + LoRA)             â”‚
â”‚                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª         â”‚
â”‚  models/ft_full/                           â”‚
â”‚  models/ft_lora/                           â”‚
â”‚  models/ft_qlora/                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           æ¨è«–ãƒ»è©•ä¾¡ãƒ¬ã‚¤ãƒ¤ãƒ¼               â”‚
â”‚  â€¢ vLLM API ã‚µãƒ¼ãƒ                         â”‚
â”‚  â€¢ app_rag_compare.py                      â”‚
â”‚  â€¢ compare_adapters.py                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“š å¾®èª¿æ•´æ‰‹æ³•ã®èª¬æ˜

### **1. ãƒ•ãƒ«å¾®èª¿æ•´ï¼ˆFull FTï¼‰**
ãƒ¢ãƒ‡ãƒ«å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°ã™ã‚‹ã€æœ€ã‚‚æ±ç”¨çš„ã§é«˜å“è³ªãªæ‰‹æ³•ã€‚  
ãŸã ã— VRAM ã‚’å¤§ããæ¶ˆè²»ã—ã¾ã™ã€‚

å®Ÿè¡Œ:
```bash
python train_full.py
```

---

### **2. LoRA**
ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å±¤ã«ä½ãƒ©ãƒ³ã‚¯è¡Œåˆ—ï¼ˆA, Bï¼‰ã‚’æŒ¿å…¥ã—ã€  
**ãã®éƒ¨åˆ†ã ã‘å­¦ç¿’ã™ã‚‹**çœãƒ¡ãƒ¢ãƒªæ‰‹æ³•ã€‚

ç‰¹å¾´ï¼š
- æœ¬ä½“ãƒ¢ãƒ‡ãƒ«ã¯å‡çµ  
- å­¦ç¿’ãŒé«˜é€Ÿ  
- çœãƒ¡ãƒ¢ãƒª  
- Adapter é‡ã¿ãŒéå¸¸ã«å°ã•ã„

å®Ÿè¡Œ:
```bash
python train_lora.py
```

---

### **3. QLoRA**
ãƒ¢ãƒ‡ãƒ«æœ¬ä½“ã‚’ **4bitï¼ˆNF4ï¼‰é‡å­åŒ–**ã—ã€  
LoRA ã‚¢ãƒ€ãƒ—ã‚¿éƒ¨åˆ†ã®ã¿ fp16 ã§å­¦ç¿’ã™ã‚‹æ–¹å¼ã€‚

ç‰¹å¾´ï¼š
- VRAM ä½¿ç”¨é‡ãŒæœ€å°  
- 8ã€œ16GB GPU ã§å®Ÿç”¨çš„  
- LoRA ã¨åŒç­‰ã®å“è³ª

å®Ÿè¡Œ:
```bash
python train_qlora.py
```

---

## ğŸ§ª ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ

ãƒ‡ãƒ¼ã‚¿ï¼š`data/toy_qa.jsonl`

å½¢å¼ï¼š
```json
{"question": "...", "answer": "..."}
```

---

## ğŸš€ vLLM ã«ã‚ˆã‚‹æ¨è«–

PagedAttention ã‚’ä½¿ã£ãŸé«˜ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæ¨è«–ã€‚

å®Ÿè¡Œ:
```bash
python -m vllm.entrypoints.openai.api_server --model ./models/ft_full
```

---

## ğŸ” RAG è©•ä¾¡

å®Ÿè¡Œ:
```bash
python app_rag_compare.py --docs_dir docs --question "Explain LoRA."
```

è©•ä¾¡é …ç›®ï¼š
- åŸ‹ã‚è¾¼ã¿æ€§èƒ½  
- æ¤œç´¢å“è³ª  
- å¿œç­”ã®ä¸€è²«æ€§  

---

## ğŸ§­ ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ

å®Ÿè¡Œ:
```bash
python compare_adapters.py
```

æ¯”è¼ƒå¯¾è±¡ï¼š
- Full FT  
- LoRA  
- QLoRA  

---

## ğŸ“ ãƒªãƒã‚¸ãƒˆãƒªæ§‹æˆ

```
llm_ft_tinyllama/
â”œâ”€â”€ train_full.py
â”œâ”€â”€ train_lora.py
â”œâ”€â”€ train_qlora.py
â”œâ”€â”€ compare_adapters.py
â”œâ”€â”€ app_rag_compare.py
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ ft_full/
â”‚   â”œâ”€â”€ ft_lora/
â”‚   â””â”€â”€ ft_qlora/
â”‚
â””â”€â”€ data/
    â””â”€â”€ toy_qa.jsonl
```

---

## ğŸ›  å¿…è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒª

- Python 3.10+  
- PyTorch (CUDA)  
- HuggingFace Transformers  
- PEFT  
- bitsandbytes  
- vLLM  

ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«:
```bash
pip install -r requirements.txt
```

---

## ğŸ™Œ æœ€å¾Œã«

æœ¬ãƒªãƒã‚¸ãƒˆãƒªã¯ã€TinyLlama ã‚’ç”¨ã„ãŸ LLM å¾®èª¿æ•´ç ”ç©¶ã®  
**ã‚¯ãƒªãƒ¼ãƒ³ã§æ‹¡å¼µæ€§ã®é«˜ã„ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³**ã¨ã—ã¦è¨­è¨ˆã•ã‚Œã¦ã„ã¾ã™ã€‚

