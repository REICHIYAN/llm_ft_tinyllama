# FT-Lab

A compact, reproducible toolkit for fine-tuning, evaluating, and comparing  
TinyLlama-1.1B-Chat-v1.0 using Full FT, LoRA, and QLoRA.

The project is designed for small-GPU environments, research experimentation,  
and transparent ablation studies.

---

## Features

### ðŸ”§ Fine-Tuning
- Full Fine-Tuning  
- LoRA  
- QLoRA  
- Shared training utilities (`training_utils.py`)

### ðŸ“˜ Evaluation Tools
- RAG evaluation (LlamaIndex / LangChain)  
- Retrieval-only metrics  
- Model comparison (FT / LoRA / QLoRA)  
- Local inference script (`local_hf_chat_model.py`)

### ðŸ—‚ Sample Data
- RAG document samples  
- Small QA datasets  

**Note:** Prefix Tuning is intentionally excluded.

---

## Repository Structure

```
ft_lab/
â”œâ”€â”€ app_rag_compare.py
â”œâ”€â”€ app_rag_compare_langchain.py
â”œâ”€â”€ app_rag_compare_llamaindex.py
â”œâ”€â”€ compare_adapters.py
â”œâ”€â”€ eval_models.py
â”œâ”€â”€ eval_retrieval.py
â”œâ”€â”€ local_hf_chat_model.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ training_utils.py
â”œâ”€â”€ train_full.py
â”œâ”€â”€ train_lora.py
â”œâ”€â”€ train_qlora.py
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ ft_full/
â”‚   â”œâ”€â”€ ft_lora/
â”‚   â””â”€â”€ ft_qlora/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ toy_qa.jsonl
â”‚   â””â”€â”€ sample_eval.jsonl
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ sample1.txt
â”‚   â””â”€â”€ sample2.txt	
â”‚
â””â”€â”€ examples/
     â””â”€â”€ FT-Lab.ipynb
```

---

## Fine-Tuning Scripts

### Full Fine-Tuning  
Updates all parameters.

```bash
python train_full.py
```

### LoRA  
Parameter-efficient training with injected low-rank matrices.

```bash
python train_lora.py
```

### QLoRA  
4-bit quantized base model + LoRA adapters.

```bash
python train_qlora.py
```

---

## Training Utilities

`training_utils.py` includes:

- dataset loading  
- tokenizer setup  
- model initialization  
- training arguments  
- evaluation hooks  

All training scripts share this module for consistent behavior.

---

## RAG Evaluation

### LlamaIndex Pipeline
**File:** `app_rag_compare_llamaindex.py`

```bash
python app_rag_compare.py     --docs_dir docs     --question "Explain LoRA."
```

### LangChain Pipeline
**File:** `app_rag_compare_langchain.py`  
Compatible with LangChain 0.2+ (Runnable / LCEL).

---

## Model Comparison

Compare FT / LoRA / QLoRA generations:

```bash
python compare_adapters.py
```

Outputs:
- aligned generations  
- qualitative differences  
- optional latency comparison  

---

## Retrieval-Only Metrics
```bash
python eval_retrieval.py --data data/sample_eval.jsonl
```

Metrics:
- recall@k  
- precision@k  
- hit-rate  

---

## Model Evaluation (FT / LoRA / QLoRA)
```bash
python eval_models.py --data_path data/sample_eval.jsonl
```

Metrics:
- BERTScore-F1  
- exact-match accuracy  
- relaxed-match accuracy  

---

## Sample Data

```
data/toy_qa.jsonl
data/sample_eval.jsonl
docs/sample1.txt
docs/sample2.txt
```

Useful for RAG demonstrations and baseline evaluations.

---

## Running the Colab Demo

A runnable notebook is available under:

```
examples/FT-Lab.ipynb
```

This notebook:
- uses only dummy data
- demonstrates the end-to-end pipeline
- is designed for Colab / T4 / small VRAM
- can be fully replaced with real datasets

---

## Requirements

```
torch>=2.1.0
transformers>=4.39.0
accelerate>=0.27.0
sentencepiece>=0.1.99
einops>=0.7.0

datasets>=2.18.0
peft>=0.10.0
bitsandbytes>=0.42.0

langchain>=0.2.0
langchain-openai>=0.1.0
llama-index>=0.10.0
llama-index-embeddings-huggingface
sentence-transformers

python-dotenv>=1.0.0
```

---

## Install

```bash
pip install -r requirements.txt
```
